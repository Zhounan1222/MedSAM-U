# MedSAM-U: Uncertainty-guided Multi-type Prompts Adapting for Reliable MedSAM
![The overall framework of MedSAM-U. The framework is presented through three key illustrations: (a) the training process of MPA-MedSAM, (b) a comprehensive work flow in the inference time, and (c) a simplified diagram that illustrates the user interaction process.](https://github.com/Zhounan1222/MedSAM-U/blob/main/framework.png)
 ## Paper link

 ## Abstract
The Medical Segment Anything Model (MedSAM) has shown remarkable performance in medical image segmentation, drawing significant attention in the field. However, its sensitivity to varying prompt types and locations poses challenges. This paper addresses these challenges by focusing on the development of reliable prompts that enhance MedSAM's accuracy. We introduce MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation. Specifically, we first train a Multi-Prompt Adapter integrated with MedSAM, creating MPA-MedSAM, to adapt to diverse multi-prompt inputs. We then employ uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results. In particular, a novel uncertainty-guided prompts adaptation technique is then applied automatically to derive reliable prompts and their corresponding segmentation outcomes. 
 ## Code
 The code will be released soon.
